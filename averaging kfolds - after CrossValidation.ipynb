{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import set_random_seed\n",
    "from tensorflow.python.client import device_lib\n",
    "import datetime as dt\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "import matplotlib.colors as mcolors\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "set_random_seed(7)\n",
    "from scipy.spatial import distance\n",
    "from numpy.random import choice\n",
    "import math\n",
    "from scipy import stats\n",
    "from scipy import optimize\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_saving_dir = \"cross_validation/averages/general/\"\n",
    "if not os.path.exists(general_saving_dir):\n",
    "    os.makedirs(general_saving_dir)\n",
    "    \n",
    "reward_oriented_saving_dir = \"cross_validation/averages/reward_oriented/\"\n",
    "if not os.path.exists(reward_oriented_saving_dir):\n",
    "    os.makedirs(reward_oriented_saving_dir)\n",
    "    \n",
    "no_reward_saving_dir = \"cross_validation/averages/no_reward/\"\n",
    "if not os.path.exists(no_reward_saving_dir):\n",
    "    os.makedirs(no_reward_saving_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# just for creating the final results when getting all folds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # # 3.7.19\n",
    "seq_length = 4\n",
    "with open('pd_list_full_with_rewards_original_seq4.pkl', 'rb') as f:\n",
    "    pd_list = pickle.load(f)\n",
    "    \n",
    "# # with open('huge_pd_with_rewards_original_seq4.pkl', 'rb') as f:\n",
    "# #     huge_pd = pickle.load(f)\n",
    "    \n",
    "with open('huge_pd_shuffled_with_rewards_original_seq4_SHUFFLED.pkl', 'rb') as f:\n",
    "    huge_pd = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(329,)\n",
      "(312,)\n",
      "(324,)\n"
     ]
    }
   ],
   "source": [
    "full_orig_data = pd.read_csv('DataAllSubjectsRewards.csv')\n",
    "print(full_orig_data[full_orig_data['payoff_structure']==2].user.unique().shape)\n",
    "print(full_orig_data[full_orig_data['payoff_structure']==3].user.unique().shape)\n",
    "print(full_orig_data[full_orig_data['payoff_structure']==4].user.unique().shape)\n",
    "\n",
    "data_filled_NA = full_orig_data.fillna(-1)\n",
    "data_noNA = full_orig_data.dropna()\n",
    "# notice that here is where I choose if feeding full data with blanks, or without blanks\n",
    "# without blanks:\n",
    "# data_payoff2 = data_noNA[data_noNA['payoff_structure']==2]\n",
    "# data_payoff3 = data_noNA[data_noNA['payoff_structure']==3]\n",
    "# data_payoff4 = data_noNA[data_noNA['payoff_structure']==4]\n",
    "# with blanks:\n",
    "data_payoff2 = data_filled_NA[data_filled_NA['payoff_structure']==2]\n",
    "data_payoff3 = data_filled_NA[data_filled_NA['payoff_structure']==3]\n",
    "data_payoff4 = data_filled_NA[data_filled_NA['payoff_structure']==4]\n",
    "user_ids2 = data_payoff2.user.unique()\n",
    "user_ids3 = data_payoff3.user.unique()\n",
    "user_ids4 = data_payoff4.user.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the kfolded pars\n",
    "with open('cross_validation/payoff2_train_participants_5fold_list.pkl', 'rb') as f:\n",
    "    payoff2_train_participants_5fold_list = pickle.load(f)\n",
    "\n",
    "with open('cross_validation/payoff2_test_participants_5fold_list.pkl', 'rb') as f:\n",
    "    payoff2_test_participants_5fold_list = pickle.load(f)\n",
    "\n",
    "with open('cross_validation/payoff3_train_participants_5fold_list.pkl', 'rb') as f:\n",
    "    payoff3_train_participants_5fold_list = pickle.load(f)\n",
    "\n",
    "with open('cross_validation/payoff3_test_participants_5fold_list.pkl', 'rb') as f:\n",
    "    payoff3_test_participants_5fold_list = pickle.load(f)\n",
    "\n",
    "with open('cross_validation/payoff4_train_participants_5fold_list.pkl', 'rb') as f:\n",
    "    payoff4_train_participants_5fold_list = pickle.load(f)\n",
    "    \n",
    "with open('cross_validation/payoff4_test_participants_5fold_list.pkl', 'rb') as f:\n",
    "    payoff4_test_participants_5fold_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pars = set()\n",
    "for i in range(5):\n",
    "    fold_set_example = [*payoff2_test_participants_5fold_list[i] , *payoff3_test_participants_5fold_list[i] , *payoff4_test_participants_5fold_list[i]]\n",
    "    total_pars.update(fold_set_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prior results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Full data trained model results , over full data(aka \"All\" or \"General\")\n",
    "\n",
    "general_dir = 'cross_validation/general_model/'\n",
    "filename = 'general_model_results_5folds.pkl'\n",
    "with open(os.path.join(general_dir,filename), 'rb') as f:\n",
    "    general_folds_results = pickle.load(f)\n",
    "# persons_true_false_predictions,persons_predictions_expr,total_actual_predictions,total_predicted_correct = results\n",
    "# inquiry_results = pd.read_csv('inquiry_results_SHUFFLED_NEW.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reward oriented (qlearn generated data) trained model results, over full data (aka \"Reward_Oriented\")\n",
    "\n",
    "reward_oriented_dir = 'cross_validation/generated_data_model/'\n",
    "filename = 'simulated_data_model_results_5folds.pkl'\n",
    "with open(os.path.join(reward_oriented_dir,filename), 'rb') as f:\n",
    "    reward_oriented_folds_results = pickle.load(f)\n",
    "# reward_oriented_persons_true_false_predictions, reward_oriented_persons_predictions_expr, reward_oriented_total_actual_predictions, reward_oriented_total_predicted_correct = reward_oriented_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## No Reward trained model results, over full data (aka \"No_Reward\")\n",
    "\n",
    "no_reward_dir = 'cross_validation/no_reward_model/'\n",
    "filename = 'no_reward_model_results_5folds.pkl'\n",
    "with open(os.path.join(no_reward_dir,filename), 'rb') as f:\n",
    "    no_reward_folds_results = pickle.load(f)\n",
    "# no_reward_persons_true_false_predictions, no_reward_persons_predictions_expr, no_reward_total_actual_predictions, no_reward_total_predicted_correct = no_reward_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each fold is :\n",
    "# persons_true_false_predictions,persons_predictions_expr,total_actual_predictions,total_predicted_correct, persons_activations = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_users_actual_choices(all_times_compare_df_payoff):\n",
    "    \"\"\"\n",
    "    get a DataFrame holding the participants (#) with actual choice and cluster for CERTAIN time t (all_times_compare_df_payoff index 0 is time 4 actually)\n",
    "    returns : ACTUAL choices for each cluster \n",
    "    \"\"\"\n",
    "    clusters_choices = []\n",
    "    for cluster in all_times_compare_df_payoff.clusters.unique():\n",
    "        clusters_choices.append(list(all_times_compare_df_payoff[all_times_compare_df_payoff['clusters']==cluster].choices))\n",
    "    return clusters_choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_users_predicted_choices(pars_in_cluster_at_time_t:dict,time):\n",
    "    \"\"\"\n",
    "    get a dictionary holding the participants (#) in each cluster for CERTAIN time t \n",
    "    NOTICE THAT PARS_IN_CLUSTER ARE (1-965) and prediction_mat is (0-964), hence the participant-1\n",
    "    returns : predictions of choices for each cluster \n",
    "    \"\"\"\n",
    "    clusters_choices = []\n",
    "    for cluster in pars_in_cluster_at_time_t.keys():\n",
    "        choices = []\n",
    "        for participant in pars_in_cluster_at_time_t[cluster]:\n",
    "            choices.append(prediction_mat[participant-1,time])\n",
    "        clusters_choices.append(choices)\n",
    "    return clusters_choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corresponding_sums(payoff_cluster_sums_mat, payoff_ratios_mat):\n",
    "    \"\"\"\n",
    "    In order to get a dataframe of the clusters sums SORTED by their corresponding ratios:\n",
    "    Gets: the payoff cluster sums matrix and the payoff ratios matrix.\n",
    "    Returns: a df of the sorted sums (by their original ratios) ---> ascending=True like (same as sorted_ratios_payoff_for_pcolor).\n",
    "    \"\"\"\n",
    "    sorted_args_for_ratios_mat = pd.DataFrame(np.argsort(payoff_ratios_mat,axis=0))\n",
    "    check = payoff_cluster_sums_mat.T.copy()\n",
    "    res = []\n",
    "    for i in range(sorted_args_for_ratios_mat.shape[1]):\n",
    "        res.append(np.ravel(check[i,list(sorted_args_for_ratios_mat[i])]))\n",
    "    return pd.DataFrame(np.asmatrix(res).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lstm_acc_per_person(persons_true_false_predictions):\n",
    "    collect_not_minuses = []\n",
    "    for pred in persons_true_false_predictions:\n",
    "        if pred<0:\n",
    "            continue\n",
    "        else:\n",
    "            collect_not_minuses.append(pred)\n",
    "    return np.mean(collect_not_minuses)\n",
    "#     return np.mean(persons_true_false_predictions[4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_false_predictions(person_df, participant, payoff_qlearn_person_predictions, payoff_person_real_accuracies, payoff_length_correction):\n",
    "    \"\"\"\n",
    "    gets the person_df and the curresponding user id (participant) with the qlearning predictions (and payoff_person_real_accuracies for assertion), and returns a true/false vector of 150\n",
    "    \"\"\"\n",
    "    person_choices = person_df.choice -1\n",
    "    participant_true_false = []\n",
    "    for choice, prediction in zip(person_choices, payoff_qlearn_person_predictions[participant-1-payoff_length_correction]):\n",
    "        if choice<0:\n",
    "            participant_true_false.append(np.nan)\n",
    "        else:\n",
    "            if choice == prediction:\n",
    "                participant_true_false.append(1)\n",
    "            else:\n",
    "                participant_true_false.append(0)\n",
    "\n",
    "    participant_true_false_np = np.asarray(participant_true_false)\n",
    "    assert(np.nanmean(participant_true_false_np)==payoff_person_real_accuracies[participant-1-payoff_length_correction]),\"accuracy is wrong\"\n",
    "    participant_true_false_np = np.nan_to_num(participant_true_false_np, -1)\n",
    "    return participant_true_false_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : adjust this?\n",
    "def get_avg_acc_per_time(payoff_person_true_false_predictions_matrix):\n",
    "    \"\"\"\n",
    "    gets a payoff_person_true_false_predictions_matrix -  which can be either qlearning or lstm \n",
    "    return a vector of 150, each entrance is the avg accuracy across all participants in that payoff\n",
    "    # usage:\n",
    "    # get_avg_acc_per_time(payoff2_persons_true_false_predictions_lstm)\n",
    "    \"\"\"\n",
    "    payoff_all_pars_avg_acc_per_time = []\n",
    "    for t in range(payoff_person_true_false_predictions_matrix.shape[1]):\n",
    "        collect_not_minuses = []\n",
    "        for pred in payoff_person_true_false_predictions_matrix[:,t]:\n",
    "            if pred<0:\n",
    "                continue\n",
    "            else:\n",
    "                collect_not_minuses.append(pred)\n",
    "        payoff_all_pars_avg_acc_per_time.append(np.mean(collect_not_minuses))\n",
    "    \n",
    "    return payoff_all_pars_avg_acc_per_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Very important :    \n",
    "\n",
    "# Each fold has it's own participants, but concatenating all folds one after another DOES NOT results in the true order of the participnats,\n",
    "# So I can't just divide regulary ! NEED TO MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_true_false_correct_order(payoff2_test_participants_5fold_list, payoff3_test_participants_5fold_list, payoff4_test_participants_5fold_list, model_folds_results):\n",
    "\n",
    "    folded_tests_sets = []\n",
    "    dfs= []\n",
    "    for test_2, test_3, test_4, results in zip(payoff2_test_participants_5fold_list, payoff3_test_participants_5fold_list, payoff4_test_participants_5fold_list, model_folds_results):\n",
    "        fold_set = [*test_2 , *test_3 , *test_4]\n",
    "        folded_tests_sets.append(fold_set)\n",
    "        general_persons_true_false_predictions, general_persons_predictions_expr,\\\n",
    "        general_total_actual_predictions, general_total_predicted_correct, general_persons_activations = results\n",
    "\n",
    "        cur_df = pd.DataFrame(general_persons_true_false_predictions, index=fold_set)\n",
    "\n",
    "        dfs.append(cur_df)\n",
    "        \n",
    "    return pd.concat(dfs).sort_index().to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy per time (average accuracy of all participants per time)\n",
    " - the accuracy per time is the average of this time over all folds\n",
    " - producing here true_false_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_all_folds_true_false = produce_true_false_correct_order(payoff2_test_participants_5fold_list, payoff3_test_participants_5fold_list, payoff4_test_participants_5fold_list, general_folds_results)\n",
    "        \n",
    "\n",
    "reward_oriented_all_folds_true_false = produce_true_false_correct_order(payoff2_test_participants_5fold_list, payoff3_test_participants_5fold_list, payoff4_test_participants_5fold_list, reward_oriented_folds_results)\n",
    "\n",
    "\n",
    "no_reward_all_folds_true_false = produce_true_false_correct_order(payoff2_test_participants_5fold_list, payoff3_test_participants_5fold_list, payoff4_test_participants_5fold_list, no_reward_folds_results)\n",
    "        \n",
    "\n",
    "assert(np.array_equal(general_all_folds_true_false, reward_oriented_all_folds_true_false)==False)\n",
    "assert(np.array_equal(general_all_folds_true_false, no_reward_all_folds_true_false)==False) \n",
    "assert(np.array_equal(no_reward_all_folds_true_false, reward_oriented_all_folds_true_false)==False)\n",
    "    \n",
    "# divide General lstm true/false matrix to payoffs\n",
    "general_payoff2_persons_true_false_predictions_lstm = np.asmatrix(general_all_folds_true_false[0:329])\n",
    "general_payoff3_persons_true_false_predictions_lstm = np.asmatrix(general_all_folds_true_false[329:641])\n",
    "general_payoff4_persons_true_false_predictions_lstm = np.asmatrix(general_all_folds_true_false[641:965])\n",
    "##############################################\n",
    "# divide Reward-Oriented lstm true/false matrix to payoffs\n",
    "reward_oriented_payoff2_persons_true_false_predictions_lstm = np.asmatrix(reward_oriented_all_folds_true_false[0:329])\n",
    "reward_oriented_payoff3_persons_true_false_predictions_lstm = np.asmatrix(reward_oriented_all_folds_true_false[329:641])\n",
    "reward_oriented_payoff4_persons_true_false_predictions_lstm = np.asmatrix(reward_oriented_all_folds_true_false[641:965])\n",
    "##################################################\n",
    "# divide No-Reward lstm true/false matrix to payoffs\n",
    "no_reward_payoff2_persons_true_false_predictions_lstm = np.asmatrix(no_reward_all_folds_true_false[0:329])\n",
    "no_reward_payoff3_persons_true_false_predictions_lstm = np.asmatrix(no_reward_all_folds_true_false[329:641])\n",
    "no_reward_payoff4_persons_true_false_predictions_lstm = np.asmatrix(no_reward_all_folds_true_false[641:965])\n",
    "\n",
    "\n",
    "general_all_folds_all_payoffs_true_false = general_all_folds_true_false, general_payoff2_persons_true_false_predictions_lstm, general_payoff3_persons_true_false_predictions_lstm, general_payoff4_persons_true_false_predictions_lstm\n",
    "with open(os.path.join(general_saving_dir, 'general_all_folds_all_payoffs_true_false.pkl'), 'wb') as f:\n",
    "    pickle.dump(general_all_folds_all_payoffs_true_false, f)\n",
    "\n",
    "\n",
    "reward_oriented_all_folds_all_payoffs_true_false = reward_oriented_all_folds_true_false, reward_oriented_payoff2_persons_true_false_predictions_lstm, reward_oriented_payoff3_persons_true_false_predictions_lstm, reward_oriented_payoff4_persons_true_false_predictions_lstm\n",
    "with open(os.path.join(reward_oriented_saving_dir, 'reward_oriented_all_folds_all_payoffs_true_false.pkl'), 'wb') as f:\n",
    "    pickle.dump(reward_oriented_all_folds_all_payoffs_true_false, f)\n",
    "    \n",
    "\n",
    "no_reward_all_folds_all_payoffs_true_false = no_reward_all_folds_true_false, no_reward_payoff2_persons_true_false_predictions_lstm, no_reward_payoff3_persons_true_false_predictions_lstm, no_reward_payoff4_persons_true_false_predictions_lstm\n",
    "with open(os.path.join(no_reward_saving_dir, 'no_reward_all_folds_all_payoffs_true_false.pkl'), 'wb') as f:\n",
    "    pickle.dump(no_reward_all_folds_all_payoffs_true_false, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy per participant\n",
    " - notice that altough we get all 965 participants, they were trained and tested with 5 different models !\n",
    " - the accuracy of each participant is the accuracy that we recieved for a certain fold - since as said, the participant was not tested in other folds as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# first extract all accuracies #\n",
    "################################\n",
    "general_all_fold_lstm_accs = []\n",
    "general_payoff2_lstm_accs = []\n",
    "for user_ind in user_ids2:\n",
    "    cur_accs = extract_lstm_acc_per_person(general_all_folds_true_false[user_ind-1])\n",
    "    general_payoff2_lstm_accs.append(cur_accs)\n",
    "    general_all_fold_lstm_accs.append(cur_accs)\n",
    "    \n",
    "general_payoff3_lstm_accs = []\n",
    "for user_ind in user_ids3:\n",
    "    cur_accs = extract_lstm_acc_per_person(general_all_folds_true_false[user_ind-1])\n",
    "    general_payoff3_lstm_accs.append(cur_accs)\n",
    "    general_all_fold_lstm_accs.append(cur_accs)\n",
    "    \n",
    "general_payoff4_lstm_accs = []\n",
    "for user_ind in user_ids4:\n",
    "    cur_accs = extract_lstm_acc_per_person(general_all_folds_true_false[user_ind-1])\n",
    "    general_payoff4_lstm_accs.append(cur_accs)\n",
    "    general_all_fold_lstm_accs.append(cur_accs)\n",
    "\n",
    "#############################\n",
    "\n",
    "reward_oriented_all_fold_lstm_accs = []\n",
    "reward_oriented_payoff2_lstm_accs = []\n",
    "for user_ind in user_ids2:\n",
    "    cur_accs = extract_lstm_acc_per_person(reward_oriented_all_folds_true_false[user_ind-1])\n",
    "    reward_oriented_payoff2_lstm_accs.append(cur_accs)\n",
    "    reward_oriented_all_fold_lstm_accs.append(cur_accs)\n",
    "    \n",
    "reward_oriented_payoff3_lstm_accs = []\n",
    "for user_ind in user_ids3:\n",
    "    cur_accs = extract_lstm_acc_per_person(reward_oriented_all_folds_true_false[user_ind-1])\n",
    "    reward_oriented_payoff3_lstm_accs.append(cur_accs)\n",
    "    reward_oriented_all_fold_lstm_accs.append(cur_accs)\n",
    "    \n",
    "reward_oriented_payoff4_lstm_accs = []\n",
    "for user_ind in user_ids4:\n",
    "    cur_accs = extract_lstm_acc_per_person(reward_oriented_all_folds_true_false[user_ind-1])\n",
    "    reward_oriented_payoff4_lstm_accs.append(cur_accs)\n",
    "    reward_oriented_all_fold_lstm_accs.append(cur_accs)\n",
    "    \n",
    "################################\n",
    "\n",
    "\n",
    "no_reward_all_fold_lstm_accs = []\n",
    "no_reward_payoff2_lstm_accs = []\n",
    "for user_ind in user_ids2:\n",
    "    cur_accs = extract_lstm_acc_per_person(no_reward_all_folds_true_false[user_ind-1])\n",
    "    no_reward_payoff2_lstm_accs.append(cur_accs)\n",
    "    no_reward_all_fold_lstm_accs.append(cur_accs)\n",
    "    \n",
    "no_reward_payoff3_lstm_accs = []\n",
    "for user_ind in user_ids3:\n",
    "    cur_accs = extract_lstm_acc_per_person(no_reward_all_folds_true_false[user_ind-1])\n",
    "    no_reward_payoff3_lstm_accs.append(cur_accs)\n",
    "    no_reward_all_fold_lstm_accs.append(cur_accs)\n",
    "    \n",
    "no_reward_payoff4_lstm_accs = []\n",
    "for user_ind in user_ids4:\n",
    "    cur_accs = extract_lstm_acc_per_person(no_reward_all_folds_true_false[user_ind-1])\n",
    "    no_reward_payoff4_lstm_accs.append(cur_accs)\n",
    "    no_reward_all_fold_lstm_accs.append(cur_accs)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "#################\n",
    "# general model #\n",
    "###########################################################################\n",
    "  \n",
    "assert(general_payoff2_lstm_accs!=general_payoff3_lstm_accs)\n",
    "assert(general_payoff2_lstm_accs!=general_payoff4_lstm_accs)\n",
    "assert(general_payoff3_lstm_accs!=general_payoff4_lstm_accs)\n",
    "\n",
    "\n",
    "general_all_accuracy_per_par = general_all_fold_lstm_accs, general_payoff2_lstm_accs, general_payoff3_lstm_accs, general_payoff4_lstm_accs\n",
    "with open(os.path.join(general_saving_dir, 'general_all_accuracy_per_par.pkl'), 'wb') as f:\n",
    "    pickle.dump(general_all_accuracy_per_par, f)\n",
    "############################################################################\n",
    "\n",
    "\n",
    "\n",
    "#########################\n",
    "# reward_oriented model #\n",
    "###########################################################################\n",
    "   \n",
    "assert(reward_oriented_payoff2_lstm_accs!=reward_oriented_payoff3_lstm_accs)\n",
    "assert(reward_oriented_payoff2_lstm_accs!=reward_oriented_payoff4_lstm_accs)\n",
    "assert(reward_oriented_payoff3_lstm_accs!=reward_oriented_payoff4_lstm_accs)\n",
    "\n",
    "reward_oriented_all_accuracy_per_par = reward_oriented_all_fold_lstm_accs, reward_oriented_payoff2_lstm_accs, reward_oriented_payoff3_lstm_accs, reward_oriented_payoff4_lstm_accs\n",
    "with open(os.path.join(reward_oriented_saving_dir, 'reward_oriented_all_accuracy_per_par.pkl'), 'wb') as f:\n",
    "    pickle.dump(reward_oriented_all_accuracy_per_par, f)\n",
    "############################################################################\n",
    "\n",
    "\n",
    "#########################\n",
    "# no_reward model #\n",
    "###########################################################################\n",
    "    \n",
    "assert(no_reward_payoff2_lstm_accs!=no_reward_payoff3_lstm_accs)\n",
    "assert(no_reward_payoff2_lstm_accs!=no_reward_payoff4_lstm_accs)\n",
    "assert(no_reward_payoff3_lstm_accs!=no_reward_payoff4_lstm_accs)\n",
    "\n",
    "no_reward_all_accuracy_per_par = no_reward_all_fold_lstm_accs, no_reward_payoff2_lstm_accs, no_reward_payoff3_lstm_accs, no_reward_payoff4_lstm_accs\n",
    "with open(os.path.join(no_reward_saving_dir, 'no_reward_all_accuracy_per_par.pkl'), 'wb') as f:\n",
    "    pickle.dump(no_reward_all_accuracy_per_par, f)\n",
    "############################################################################\n",
    "\n",
    "assert(no_reward_payoff2_lstm_accs!=general_payoff2_lstm_accs)\n",
    "assert(no_reward_payoff2_lstm_accs!=general_payoff3_lstm_accs)\n",
    "assert(no_reward_payoff2_lstm_accs!=general_payoff4_lstm_accs)\n",
    "assert(no_reward_payoff2_lstm_accs!=reward_oriented_payoff2_lstm_accs)\n",
    "assert(no_reward_payoff2_lstm_accs!=reward_oriented_payoff3_lstm_accs)\n",
    "assert(no_reward_payoff2_lstm_accs!=reward_oriented_payoff4_lstm_accs)\n",
    "\n",
    "assert(no_reward_payoff3_lstm_accs!=general_payoff2_lstm_accs)\n",
    "assert(no_reward_payoff3_lstm_accs!=general_payoff3_lstm_accs)\n",
    "assert(no_reward_payoff3_lstm_accs!=general_payoff4_lstm_accs)\n",
    "assert(no_reward_payoff3_lstm_accs!=reward_oriented_payoff2_lstm_accs)\n",
    "assert(no_reward_payoff3_lstm_accs!=reward_oriented_payoff3_lstm_accs)\n",
    "assert(no_reward_payoff3_lstm_accs!=reward_oriented_payoff4_lstm_accs)\n",
    "\n",
    "assert(no_reward_payoff4_lstm_accs!=general_payoff2_lstm_accs)\n",
    "assert(no_reward_payoff4_lstm_accs!=general_payoff3_lstm_accs)\n",
    "assert(no_reward_payoff4_lstm_accs!=general_payoff4_lstm_accs)\n",
    "assert(no_reward_payoff4_lstm_accs!=reward_oriented_payoff2_lstm_accs)\n",
    "assert(no_reward_payoff4_lstm_accs!=reward_oriented_payoff3_lstm_accs)\n",
    "assert(no_reward_payoff4_lstm_accs!=reward_oriented_payoff4_lstm_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
